"""
SIEM Writer service.

Читает нормализованные + отфильтрованные события из Redis Stream
`siem:filtered` и пишет их в таблицу ClickHouse `siem.events`
с колонками:

  ts DateTime
  event_id String
  category String
  subcategory String
  src_ip IPv4
  dst_ip IPv4
  severity String
  message String

Все настройки берутся из переменных окружения SIEM_*.
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
import signal
import sys
import uuid
import ipaddress
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, Dict, List, Tuple

import redis.asyncio as redis
from clickhouse_driver import Client

from common.logging import configure_logging

STREAM_KEY_FILTERED = "siem:filtered"
WRITER_LAST_ID_KEY = "siem:writer:last_id"


@dataclass
class WriterSettings:
    # Redis
    redis_host: str = os.getenv("SIEM_REDIS_HOST", "127.0.0.1")
    redis_port: int = int(os.getenv("SIEM_REDIS_PORT", "6379"))
    redis_db: int = int(os.getenv("SIEM_REDIS_DB", "0"))
    redis_password: str | None = os.getenv("SIEM_REDIS_PASSWORD") or None

    # ClickHouse
    ch_host: str = os.getenv("SIEM_CH_HOST", "127.0.0.1")
    ch_port: int = int(os.getenv("SIEM_CH_PORT", "9000"))
    ch_user: str = os.getenv("SIEM_CH_USER", "siem_app")
    ch_password: str = os.getenv("SIEM_CH_PASSWORD", "")
    ch_db: str = os.getenv("SIEM_CH_DB", "siem")

    # Writer behaviour
    batch_size: int = int(os.getenv("SIEM_WRITER_BATCH_SIZE", "200"))
    poll_interval_ms: int = int(os.getenv("SIEM_WRITER_POLL_INTERVAL_MS", "500"))


def _parse_ts(doc: Dict[str, Any]) -> datetime:
    """Пытаемся взять время события из нормализованного документа."""
    ts_raw = doc.get("ts") or doc.get("@timestamp")
    if isinstance(ts_raw, str):
        try:
            # поддерживаем ISO8601 с "Z"
            if ts_raw.endswith("Z"):
                ts_raw = ts_raw.replace("Z", "+00:00")
            return datetime.fromisoformat(ts_raw).astimezone(timezone.utc)
        except Exception:
            pass
    return datetime.now(timezone.utc)


def _ip_to_int(ip_str: str | None) -> int:
    if not ip_str:
        return 0
    try:
        return int(ipaddress.IPv4Address(ip_str))
    except Exception:
        return 0


class WriterWorker:
    def __init__(self, settings: WriterSettings) -> None:
        self.settings = settings
        self.log = logging.getLogger("siem.writer")
        self.redis: redis.Redis | None = None
        self.ch: Client | None = None
        self._stopping = False

    async def init(self) -> None:
        self.log.info(
            "WriterWorker initialized",
            extra={
                "filtered_stream": STREAM_KEY_FILTERED,
                "batch_size": self.settings.batch_size,
            },
        )

        self.redis = redis.Redis(
            host=self.settings.redis_host,
            port=self.settings.redis_port,
            db=self.settings.redis_db,
            password=self.settings.redis_password,
            encoding="utf-8",
            decode_responses=True,
        )

        self.ch = Client(
            host=self.settings.ch_host,
            port=self.settings.ch_port,
            user=self.settings.ch_user,
            password=self.settings.ch_password,
            database=self.settings.ch_db,
            send_receive_timeout=10,
        )

    async def _load_last_id(self) -> str:
        assert self.redis is not None
        last_id = await self.redis.get(WRITER_LAST_ID_KEY)
        return last_id or "0-0"

    async def _save_last_id(self, last_id: str) -> None:
        assert self.redis is not None
        await self.redis.set(WRITER_LAST_ID_KEY, last_id)

    def _build_row(self, fields: Dict[str, Any]) -> Tuple[Any, ...]:
        """
        Преобразует запись из Redis Stream в строку для ClickHouse.

        Ожидается, что в fields есть ключ "data" с JSON нормализованного события.
        """
        raw = fields.get("data")
        if raw is None:
            raise ValueError("Missing 'data' field in stream record")

        doc = json.loads(raw)

        # event.* структура
        ev = doc.get("event", {}) or {}
        src = doc.get("source", {}) or {}
        dst = doc.get("destination", {}) or {}

        ts = _parse_ts(doc)
        event_id = ev.get("id") or str(uuid.uuid4())

        category = ev.get("category") or ""
        # В UEM event.type — хорошее поле для подкатегории
        subcategory = ev.get("type") or ""

        src_ip_int = _ip_to_int(src.get("ip") or doc.get("src_ip"))
        dst_ip_int = _ip_to_int(dst.get("ip") or doc.get("dst_ip"))

        severity = (
            str(ev.get("severity"))
            if ev.get("severity") is not None
            else str(doc.get("severity") or "info")
        )

        message = (
            doc.get("message")
            or ev.get("original")
            or ev.get("message")
            or doc.get("event_original")
            or ""
        )

        return (
            ts,
            event_id,
            category,
            subcategory,
            src_ip_int,
            dst_ip_int,
            severity,
            message,
        )

    def _flush_rows(self, rows: List[Tuple[Any, ...]]) -> None:
        assert self.ch is not None
        sql = """
        INSERT INTO siem.events
        (
          ts,
          event_id,
          category,
          subcategory,
          src_ip,
          dst_ip,
          severity,
          message
        )
        VALUES
        """
        self.ch.execute(sql, rows)
        self.log.info(
            "Writer batch inserted",
            extra={"events_read": len(rows), "rows_inserted": len(rows)},
        )

    async def run(self) -> None:
        assert self.redis is not None
        last_id = await self._load_last_id()
        self.log.info("Writer starting", extra={"last_id": last_id})

        while not self._stopping:
            streams = {STREAM_KEY_FILTERED: last_id}
            resp = await self.redis.xread(
                streams,
                count=self.settings.batch_size,
                block=self.settings.poll_interval_ms,
            )

            if not resp:
                continue

            _, records = resp[0]

            rows: List[Tuple[Any, ...]] = []
            max_id = last_id

            for rec_id, fields in records:
                max_id = rec_id
                try:
                    row = self._build_row(fields)
                    rows.append(row)
                except Exception as exc:  # логируем, но не останавливаем поток
                    self.log.exception(
                        "Failed to build row from record",
                        extra={"rec_id": rec_id, "fields": fields, "error": str(exc)},
                    )

            if rows:
                self._flush_rows(rows)
                await self._save_last_id(max_id)
                last_id = max_id

        self.log.info("Writer stopped gracefully")

    def stop(self) -> None:
        self._stopping = True


async def main() -> None:
    configure_logging(service_name="siem-writer")
    settings = WriterSettings()
    worker = WriterWorker(settings)
    await worker.init()

    loop = asyncio.get_running_loop()

    def _handle_signal(signame: str) -> None:
        worker.stop()

    for signame in ("SIGINT", "SIGTERM"):
        loop.add_signal_handler(getattr(signal, signame), _handle_signal, signame)

    await worker.run()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        sys.exit(0)
